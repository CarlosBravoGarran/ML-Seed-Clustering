{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos Bravo Garrán - 100474964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Seed Clustering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import matplotlib.cm as cm\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Load the dataset__\n",
    "\n",
    "Load the seed dataset from a CSV file. \n",
    "\n",
    "The features are stored in `X`, and the target class labels are stored in `y`.\n",
    "\n",
    "Add seed variable for random states (`100474964`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/semillas.csv')\n",
    "X = data.drop(columns=['clase'])\n",
    "y = data['clase']\n",
    "\n",
    "seed = 100474964\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. Comparison of Scalers__\n",
    "\n",
    "This section identifies the most appropriate scaler for the seed dataset before applying clustering algorithms. Scaling ensures that all features contribute equally to the distance calculations.\n",
    "\n",
    "Three scalers are compared:\n",
    "- MinMaxScaler\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "\n",
    "The scaled data is projected into 2D using PCA for visual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'StandardScaler': StandardScaler()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "for i, (name, scaler) in enumerate(scalers.items(), 1):\n",
    "    pipeline = make_pipeline(scaler, PCA(n_components=2, random_state=seed))\n",
    "    \n",
    "    X_pca = pipeline.fit_transform(X)\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, s=50)\n",
    "    plt.title(f'{name} + PCA')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.suptitle('Comparison of Scalers with PCA (By Seed Class)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Best Scaler Selection\n",
    "\n",
    "After observing the PCA plots:\n",
    "\n",
    "- **MinMaxScaler**: The data points are well distributed with moderate separation between different seed classes. Although some overlap exists, the distribution appears balanced and suitable for clustering.\n",
    "- **RobustScaler**: There is a good separation of classes, but the spread of the data is larger, which may not be ideal for density-based methods.\n",
    "- **StandardScaler**: While resistant to outliers, the classes are not well separated, making clustering more difficult.\n",
    "\n",
    "The scaler selected is **MinMaxScaler** because it provides a balanced and homogeneous distribution of the data, facilitating the identification of clusters without introducing large variations in scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Variance Explained by PCA\n",
    "\n",
    "To ensure that the 2D projection using PCA retains enough information from the original dataset, calculate, after applying each scaler, the variance explained by the two principal components and the total variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_ratios = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=2, random_state=seed)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    variance_explained = np.sum(pca.explained_variance_ratio_)\n",
    "    variance_ratios[name] = {\n",
    "        'PC1': pca.explained_variance_ratio_[0],\n",
    "        'PC2': pca.explained_variance_ratio_[1],\n",
    "        'Total': variance_explained\n",
    "    }\n",
    "\n",
    "variance_table = pd.DataFrame.from_dict(variance_ratios, orient='index')\n",
    "variance_table.index.name = 'Scaler'\n",
    "variance_table.reset_index(inplace=True)\n",
    "\n",
    "display(variance_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that all three scalers achieve a high variance explanation (>85%), indicating that the 2D PCA projection is representative of the original dataset in every case.\n",
    "\n",
    "Among them, **MinMaxScaler** achieves the highest variance explained, with 91.81%.\n",
    "\n",
    "It's confirmed that using **MinMaxScaler** is appropriate, as it retains the largest proportion of the original data variance after dimensionality reduction. Therefore, MinMaxScaler is selected for the clustering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Scaling and PCA\n",
    "The dataset was scaled using the selecter scaler (**MinMaxScaler**) and reduced to two dimensions using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PCA(n_components=2, random_state=seed)\n",
    ")\n",
    "\n",
    "X_final_pca = pipeline.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X_final_pca[:, 0], X_final_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "plt.title('MinMaxScaler + PCA (Final Preprocessing)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('MinMaxScaler + PCA', fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. Clustering__\n",
    "\n",
    "Apply unsupervised clustering techniques on the PCA-transformed seed dataset to identify natural groupings:\n",
    "\n",
    "- K-Means\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN\n",
    "\n",
    "For each method,the key hyperparameters are tuned, the results visualized and their performance compared to determine which best captures the structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3.1 K-means clustering__\n",
    "\n",
    "Based on the visual inspection of the PCA plot, k = 3 is initially selected as a reasonable number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Model creation and training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_kmeans = KMeans(\n",
    "    n_clusters=3,\n",
    "    n_init=25,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "modelo_kmeans.fit(X_final_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Cluster prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelo_kmeans.predict(X_final_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Cluster Visualization and Evaluation__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['yellow', 'purple', 'green', 'orange', 'purple']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "for cluster in np.unique(y_pred):\n",
    "    ax.scatter(\n",
    "        x = X_final_pca[y_pred == cluster, 0],\n",
    "        y = X_final_pca[y_pred == cluster, 1],\n",
    "        color = colors[cluster],\n",
    "        label = f\"Cluster {cluster}\",\n",
    "        edgecolor = 'black',\n",
    "        marker = 'o'\n",
    "    )\n",
    "\n",
    "ax.scatter(\n",
    "    x = modelo_kmeans.cluster_centers_[:, 0],\n",
    "    y = modelo_kmeans.cluster_centers_[:, 1],\n",
    "    c = 'red',\n",
    "    s = 200,\n",
    "    marker = '*',\n",
    "    label = 'Centroids'\n",
    ")\n",
    "\n",
    "ax.set_title('Clusters generated by KMeans (K=3)')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Means clustering with k = 3 produced three clearly defined groups in the PCA-transformed space.\n",
    "\n",
    "- Each cluster is compact and well-separated from the others, with minimal overlap between groups.\n",
    "- The centroids are located close to the center of each cluster, indicating that the algorithm has correctly captured the dense regions of the data.\n",
    "- The overall structure suggests that the data naturally forms three main groups, supporting the choice of k = 3.\n",
    "- Although the clusters are not perfectly spherical, the distribution around each centroid is balanced and coherent.\n",
    "\n",
    "In conclusion, the visual inspection confirms that K-Means with k = 3 is a good fit for the structure observed in the seed dataset.\n",
    "\n",
    "Even so, other values of k will be tested to ensure robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Evaluation of K-Means with Different Values of k__\n",
    "\n",
    "In this section, the performance of the K-Means algorithm is evaluated using different values of k (number of clusters). This helps identify the optimal number of clusters that best represents the structure of the data.\n",
    "\n",
    "Plots are generated for k = 2 and k = 4, comparing the distribution of data in the PCA space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Results for K = 2\n",
    "y_predict_2 = KMeans(n_clusters=2, n_init=25, random_state=seed).fit_predict(X=X_final_pca)\n",
    "ax[0].scatter(\n",
    "    x = X_final_pca[:, 0],\n",
    "    y = X_final_pca[:, 1],\n",
    "    c = y_predict_2,\n",
    "    marker = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "ax[0].set_title('KMeans with K=2')\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Results for K = 4\n",
    "y_predict_4 = KMeans(n_clusters=4, n_init=25, random_state=seed).fit_predict(X=X_final_pca)\n",
    "ax[1].scatter(\n",
    "    x = X_final_pca[:, 0],\n",
    "    y = X_final_pca[:, 1],\n",
    "    c = y_predict_4,\n",
    "    marker = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "ax[1].set_title('KMeans with K=4')\n",
    "ax[1].set_xlabel('PC1')\n",
    "ax[1].set_ylabel('PC2')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- With **k = 2**, the data is divided into two large groups, losing important internal structures.\n",
    "- With **k = 4**, the clustering tends to oversegment the data, creating small clusters that do not correspond to natural groupings.\n",
    "- With **k = 3**, the data is divided into three compact and well-separated groups, without significant overlap or oversegmentation.\n",
    "\n",
    "The visual comparison confirms that **k = 3** is the most appropriate number of clusters for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Elbow Method__\n",
    "\n",
    "To support the selection of the number of clusters, the elbow method is applied, which analyzes how the inertia decreases as the number of clusters increases.\n",
    "\n",
    "The optimal number of clusters is identified at the point where adding an additional cluster no longer significantly reduces the inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "K = range(1, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X_final_pca)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of inertia facing the number of clusters shows a clear \"elbow\" at k = 3, reconfirming that k = 3 is the most appropriate number of clusters for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Silhouette Method__\n",
    "\n",
    "The silhouette coefficient measures how similar points within the same cluster are compared to those in other clusters. The closer to 1, the better defined the cluster.\n",
    "\n",
    "We will calculate and visualize the average silhouette coefficient for different values of k (from 2 to 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "K = range(2, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=25, random_state=seed)\n",
    "    labels = kmeans.fit_predict(X_final_pca)\n",
    "    score = silhouette_score(X_final_pca, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(K, silhouette_scores, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Silhouette Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that k = 2 has the highest silhouette coefficient (~0.56), suggesting good separation between two groups. However, this may be because the classes are divided into two large generic groups, ignoring more natural subdivisions.\n",
    "\n",
    "This differs from the choice we made earlier of k = 3, so to clarify, we will generate individual silhouette plots for k = 2 to k = 5 to see if the clusters are well-defined and how consistent the assignments are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [2, 3, 4, 5]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, k in enumerate(Ks):\n",
    "    ax = axes[idx]\n",
    "    kmeans = KMeans(n_clusters=k, n_init=25, random_state=seed)\n",
    "    cluster_labels = kmeans.fit_predict(X_final_pca)\n",
    "\n",
    "    silhouette_vals = silhouette_samples(X_final_pca, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        ith_cluster_silhouette_vals = silhouette_vals[cluster_labels == i]\n",
    "        ith_cluster_silhouette_vals.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_vals.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_vals,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10 \n",
    "\n",
    "    avg_score = silhouette_score(X_final_pca, cluster_labels)\n",
    "    ax.text(0.7, 0.9, f'Avg Silhouette: {avg_score:.3f}', transform=ax.transAxes, color=\"red\", fontsize=10)\n",
    "\n",
    "\n",
    "    ax.axvline(x=avg_score, color=\"red\", linestyle=\"--\")\n",
    "    ax.set_title(f'Silhouette plot for K = {k}')\n",
    "    ax.set_xlabel('Silhouette coefficient values')\n",
    "    ax.set_ylabel('Cluster label')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([-0.1, 1])\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Number of Clusters (k) | Average Silhouette | Analysis                                                                 |\n",
    "|------------------------|--------------------|-------------------------------------------------------------------------------|\n",
    "| 2                      | ~0.56           | Clear and compact structure, but may be an excessive simplification          |\n",
    "| 3                      | ~0.51           | Good separation, consistent with the actual number of classes (3)            |\n",
    "| 4                      | ~0.43           | Overclustering, appearance of small clusters and overlap between groups      |\n",
    "| 5                      | ~0.40           | Worse cohesion, some artificial clusters and poor point assignment           |\n",
    "\n",
    "\n",
    "The silhouette plots show that the values of n_clusters = 4 and 5 are a poor choice due to below-average silhouette scores and the appearance of clusters with very unequal sizes.\n",
    "\n",
    "On the other hand, the silhouette analysis is more ambiguous when comparing k = 2 and k = 3, as we have seen before. The value of k = 2 presents the highest average silhouette coefficient (~0.56), and its clusters are clearly separated, although the grouping may be too general, hiding more detailed structures. This can also be seen in the plot, where one of the clusters (cluster 0) is larger and encompasses multiple subsets.\n",
    "\n",
    "With k = 3, the average silhouette coefficient is slightly lower (~0.51), but the three clusters exhibit good internal cohesion and relative separation, with more balanced sizes and consistent bars. Additionally, k = 3 is consistent with the results obtained previously.\n",
    "\n",
    "In summary, although k = 2 achieves the highest average score, the visual and conceptual analysis suggests that k = 3 offers a richer, more interpretable segmentation aligned with the actual structure of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3.2 Hierarchical Clustering__\n",
    "\n",
    "In this section, **hierarchical clustering** (Agglomerative Clustering) is applied to the seed dataset transformed using PCA. Different linkage methods are tested (`linkage`), and the results are analyzed both visually and quantitatively through scatter plots, dendrograms, and the silhouette coefficient.\n",
    "\n",
    "__Tested Linkage Methods__\n",
    "\n",
    "The following hierarchical linkage methods are compared:\n",
    "- **Ward**: Minimizes variance within clusters.\n",
    "- **Complete**: Maximizes the distance between the farthest points.\n",
    "- **Average**: Uses the average distance between points.\n",
    "- **Single**: Considers the minimum distance between points (may form chains).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Comparison of Methods Using Scatter Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    # Ward sólo admite distancia euclídea\n",
    "    if method == 'ward':\n",
    "        model = AgglomerativeClustering(n_clusters=3, linkage=method)\n",
    "    else:\n",
    "        model = AgglomerativeClustering(n_clusters=3, linkage=method, metric='euclidean')\n",
    "\n",
    "    y_pred = model.fit_predict(X_final_pca)\n",
    "    score = silhouette_score(X_final_pca, y_pred)\n",
    "    print(f\"Silhouette Score ({method} linkage): {score:.3f}\")\n",
    "\n",
    "    axes[i].scatter(X_final_pca[:, 0], X_final_pca[:, 1], c=y_pred, cmap='viridis', edgecolor='k', alpha=0.7)\n",
    "    axes[i].set_title(f'Linkage: {method}')\n",
    "    axes[i].set_xlabel('PC1')\n",
    "    axes[i].set_ylabel('PC2')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.suptitle('Agglomerative Clustering with Different Linkage Methods', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados muestran que el método `ward` genera agrupaciones compactas y bien separadas, con una puntuación de silueta ligeramente superior a los demás. Los métodos `average` y `complete` ofrecen buenos resultados, pero con mayor solapamiento. Por otro lado, el método `single` suele ser menos recomendable, ya que tiende a formar cadenas y clusters poco definidos, por lo que este método se descarta directamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create a dendrogram from the hierarchical clustering model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create hierarchical models with different linkage methods and construct the complete clustering tree without fixing the number of clusters, to generate dendrograms with the full structure of merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelo_hclust_complete = AgglomerativeClustering(\n",
    "    linkage  = 'complete',\n",
    "    distance_threshold = 0,\n",
    "    n_clusters         = None\n",
    ")\n",
    "modelo_hclust_complete.fit(X=X_final_pca)\n",
    "\n",
    "modelo_hclust_average = AgglomerativeClustering(\n",
    "    linkage  = 'average',\n",
    "    distance_threshold = 0,\n",
    "    n_clusters         = None\n",
    ")\n",
    "modelo_hclust_average.fit(X=X_final_pca)\n",
    "\n",
    "modelo_hclust_ward = AgglomerativeClustering(\n",
    "    linkage  = 'ward',\n",
    "    distance_threshold = 0,\n",
    "    n_clusters         = None\n",
    ")\n",
    "modelo_hclust_ward.fit(X=X_final_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Visualization of Complete Dendrograms**\n",
    "\n",
    "Complete dendrograms are generated for the `ward`, `average`, and `complete` methods, based on models trained without fixing the number of clusters (`n_clusters=None`) and with `distance_threshold=0`. These dendrograms display the entire hierarchy of merges between observations, from individual points to the total grouping.\n",
    "\n",
    "This representation allows for the analysis of the global hierarchical structure and observation of the distances at which the merges occur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
    "\n",
    "plot_dendrogram(modelo_hclust_average, labels=range(len(X_final_pca)), color_threshold=0, ax=axs[0])\n",
    "axs[0].set_title(\"Linkage average\")\n",
    "\n",
    "plot_dendrogram(modelo_hclust_complete, labels=range(len(X_final_pca)), color_threshold=0, ax=axs[1])\n",
    "axs[1].set_title(\"Linkage complete\")\n",
    "\n",
    "plot_dendrogram(modelo_hclust_ward, labels=range(len(X_final_pca)), color_threshold=0, ax=axs[2])\n",
    "axs[2].set_title(\"Linkage ward\")\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the three methods, the dendrogram generated with **`ward`** stands out for presenting a more balanced structure with more significant distance jumps at the upper levels of the tree, suggesting clearer and more natural divisions in the dataset.\n",
    "\n",
    "In contrast, the dendrograms with `average` and `complete` show a more gradual progression, without such defined cuts, which may make it harder to visually identify an optimal number of clusters.\n",
    "\n",
    "Even so, we will truncate the dendrograms to view them more clearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Truncated Dendrograms for Easier Interpretation__\n",
    "\n",
    "Since the complete dendrograms are very extensive and difficult to read, **truncated** versions are also generated using `truncate_mode='level'` and `p=5`, which show only the last levels of the hierarchical clustering.\n",
    "\n",
    "This approach focuses on the main divisions and facilitates the visual identification of relevant groupings, such as choosing a reasonable number of clusters based on the cutting height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "plot_dendrogram(modelo_hclust_average, labels=range(len(X_final_pca)), \n",
    "                truncate_mode='level', p=5, leaf_rotation=90, leaf_font_size=8, ax=axs[0])\n",
    "axs[0].set_title(\"Linkage average\")\n",
    "\n",
    "plot_dendrogram(modelo_hclust_complete, labels=range(len(X_final_pca)), \n",
    "                truncate_mode='level', p=5, leaf_rotation=90, leaf_font_size=8, ax=axs[1])\n",
    "axs[1].set_title(\"Linkage complete\")\n",
    "\n",
    "plot_dendrogram(modelo_hclust_ward, labels=range(len(X_final_pca)), \n",
    "                truncate_mode='level', p=5, leaf_rotation=90, leaf_font_size=8, ax=axs[2])\n",
    "axs[2].set_title(\"Linkage ward\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is confirmed that the **`ward`** method presents clearer and more well-defined divisions, with a final merge occurring at a significantly higher height, indicating greater separation between groups.\n",
    "\n",
    "In comparison, the **`average`** and **`complete`** methods show more gradual and less abrupt groupings, suggesting that the differences between consecutive groups are smoother and more progressive.\n",
    "\n",
    "Overall, this visualization supports the choice of `ward` as the most effective method for capturing the hierarchical structure of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Visual Cut of the Dendrogram__\n",
    "\n",
    "To determine the number of clusters, the dendrogram is visually cut at a certain height. For the Ward method, which has shown the best results, a cut is made at height 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "altura_corte = 5\n",
    "\n",
    "plot_dendrogram(\n",
    "    modelo_hclust_ward,\n",
    "    labels=range(len(X_final_pca)),\n",
    "    truncate_mode='level',\n",
    "    p=5,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=8,\n",
    "    color_threshold=altura_corte,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(\"Linkage: Ward — Height cut = 5\")\n",
    "ax.axhline(y=altura_corte, color='black', linestyle='--', label=f'Height cut = {altura_corte}')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 main clusters** are clearly identified. This approach allows for intuitive segmentation of the data, based on the largest distance jumps between consecutive merges.\n",
    "\n",
    "The cut at this height coincides with a point where a significant separation occurs in the hierarchical tree, suggesting that the natural structure of the data favors the existence of three large groups. This observation reinforces the results obtained with other methods such as K-Means and the silhouette analysis for k = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Silhouette Coefficient Method__\n",
    "\n",
    "Apply the silhouette coefficient method to find the optimal number of clusters in hierarchical clustering (linkage='ward')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_clusters = range(2, 15)\n",
    "valores_medios_silhouette = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    modelo = AgglomerativeClustering(\n",
    "        linkage    = 'ward',\n",
    "        n_clusters = n_clusters\n",
    "    )\n",
    "\n",
    "    cluster_labels = modelo.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    valores_medios_silhouette.append(silhouette_avg)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
    "ax.plot(range_n_clusters, valores_medios_silhouette, marker='o')\n",
    "ax.set_title(\"Mean evolution of silhouette indexes\")\n",
    "ax.set_xlabel('Clusters name')\n",
    "ax.set_ylabel('Siluette index mean');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The silhouette index analysis shows that the maximum value is reached for k = 2, with an average coefficient close to 0.46. From that point, the index decreases steadily, suggesting that dividing the dataset into more groups reduces cohesion within each cluster and separation between them.\n",
    "\n",
    "Although this result suggests that k = 2 is optimal from a purely mathematical perspective, this value may be too simplistic regarding the actual structure of the dataset, which originally includes three seed classes. Therefore, k = 3 or k = 4 are also considered reasonable options, especially if a more informative segmentation aligned with prior knowledge is sought.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Choice of Method and Number of Clusters__\n",
    "\n",
    "After comparing different linkage methods and analyzing the results both visually and quantitatively, it is concluded that the best hierarchical clustering method for this dataset is the **ward** method, as it provides the most compact and well-separated clusters, supported by a good silhouette index.\n",
    "\n",
    "The visual cut of the dendrogram at height 5 generates **3 clearly differentiated clusters**, which align with the most coherent results from the K-Means analysis and adequately reflect the structure of the dataset.\n",
    "\n",
    "Therefore, we select the **ward** method and a number of clusters equal to 3 as the most suitable configuration to represent the natural groupings of seeds in this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_hclust_ward = AgglomerativeClustering(\n",
    "    linkage  = 'ward',\n",
    "    n_clusters = 3\n",
    ")\n",
    "modelo_hclust_ward.fit(X=X_scaled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
