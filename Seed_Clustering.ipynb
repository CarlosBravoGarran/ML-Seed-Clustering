{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos Bravo GarrÃ¡n - 100474964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Seed Clustering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Load the dataset__\n",
    "\n",
    "Load the seed dataset from a CSV file. \n",
    "\n",
    "The features are stored in `X`, and the target class labels are stored in `y`.\n",
    "\n",
    "Add seed variable for random states (`100474964`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/semillas.csv')\n",
    "X = data.drop(columns=['clase'])\n",
    "y = data['clase']\n",
    "\n",
    "seed = 100474964\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. Comparison of Scalers__\n",
    "\n",
    "This section identifies the most appropriate scaler for the seed dataset before applying clustering algorithms. Scaling ensures that all features contribute equally to the distance calculations.\n",
    "\n",
    "Three scalers are compared:\n",
    "- MinMaxScaler\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "\n",
    "The scaled data is projected into 2D using PCA for visual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'StandardScaler': StandardScaler()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "for i, (name, scaler) in enumerate(scalers.items(), 1):\n",
    "    pipeline = make_pipeline(scaler, PCA(n_components=2, random_state=seed))\n",
    "    \n",
    "    X_pca = pipeline.fit_transform(X)\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, s=50)\n",
    "    plt.title(f'{name} + PCA')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.suptitle('Comparison of Scalers with PCA (By Seed Class)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Best Scaler Selection\n",
    "\n",
    "After observing the PCA plots:\n",
    "\n",
    "- **MinMaxScaler**: The data points are well distributed with moderate separation between different seed classes. Although some overlap exists, the distribution appears balanced and suitable for clustering.\n",
    "- **RobustScaler**: There is a good separation of classes, but the spread of the data is larger, which may not be ideal for density-based methods.\n",
    "- **StandardScaler**: While resistant to outliers, the classes are not well separated, making clustering more difficult.\n",
    "\n",
    "The scaler selected is **MinMaxScaler** because it provides a balanced and homogeneous distribution of the data, facilitating the identification of clusters without introducing large variations in scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Variance Explained by PCA\n",
    "\n",
    "To ensure that the 2D projection using PCA retains enough information from the original dataset, calculate, after applying each scaler, the variance explained by the two principal components and the total variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_ratios = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=2, random_state=seed)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    variance_explained = np.sum(pca.explained_variance_ratio_)\n",
    "    variance_ratios[name] = {\n",
    "        'PC1': pca.explained_variance_ratio_[0],\n",
    "        'PC2': pca.explained_variance_ratio_[1],\n",
    "        'Total': variance_explained\n",
    "    }\n",
    "\n",
    "variance_table = pd.DataFrame.from_dict(variance_ratios, orient='index')\n",
    "variance_table.index.name = 'Scaler'\n",
    "variance_table.reset_index(inplace=True)\n",
    "\n",
    "display(variance_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that all three scalers achieve a high variance explanation (>85%), indicating that the 2D PCA projection is representative of the original dataset in every case.\n",
    "\n",
    "Among them, **MinMaxScaler** achieves the highest variance explained, with 91.81%.\n",
    "\n",
    "It's confirmed that using **MinMaxScaler** is appropriate, as it retains the largest proportion of the original data variance after dimensionality reduction. Therefore, MinMaxScaler is selected for the clustering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Scaling and PCA\n",
    "The dataset was scaled using the selecter scaler (**MinMaxScaler**) and reduced to two dimensions using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PCA(n_components=2, random_state=seed)\n",
    ")\n",
    "\n",
    "X_final_pca = pipeline.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X_final_pca[:, 0], X_final_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "plt.title('MinMaxScaler + PCA (Final Preprocessing)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('MinMaxScaler + PCA', fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. Clustering__\n",
    "\n",
    "Apply unsupervised clustering techniques on the PCA-transformed seed dataset to identify natural groupings:\n",
    "\n",
    "- K-Means\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN\n",
    "\n",
    "For each method,the key hyperparameters are tuned, the results visualized and their performance compared to determine which best captures the structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3.1 K-means clustering__\n",
    "\n",
    "Based on the visual inspection of the PCA plot, k = 3 is initially selected as a reasonable number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Model creation and training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_kmeans = KMeans(\n",
    "    n_clusters=3,\n",
    "    n_init=25,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "modelo_kmeans.fit(X_final_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Cluster prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelo_kmeans.predict(X_final_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Cluster Visualization and Evaluation__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['yellow', 'purple', 'green', 'orange', 'purple']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "for cluster in np.unique(y_pred):\n",
    "    ax.scatter(\n",
    "        x = X_final_pca[y_pred == cluster, 0],\n",
    "        y = X_final_pca[y_pred == cluster, 1],\n",
    "        color = colors[cluster],\n",
    "        label = f\"Cluster {cluster}\",\n",
    "        edgecolor = 'black',\n",
    "        marker = 'o'\n",
    "    )\n",
    "\n",
    "ax.scatter(\n",
    "    x = modelo_kmeans.cluster_centers_[:, 0],\n",
    "    y = modelo_kmeans.cluster_centers_[:, 1],\n",
    "    c = 'red',\n",
    "    s = 200,\n",
    "    marker = '*',\n",
    "    label = 'Centroids'\n",
    ")\n",
    "\n",
    "ax.set_title('Clusters generated by KMeans (K=3)')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Means clustering with k = 3 produced three clearly defined groups in the PCA-transformed space.\n",
    "\n",
    "- Each cluster is compact and well-separated from the others, with minimal overlap between groups.\n",
    "- The centroids are located close to the center of each cluster, indicating that the algorithm has correctly captured the dense regions of the data.\n",
    "- The overall structure suggests that the data naturally forms three main groups, supporting the choice of k = 3.\n",
    "- Although the clusters are not perfectly spherical, the distribution around each centroid is balanced and coherent.\n",
    "\n",
    "In conclusion, the visual inspection confirms that K-Means with k = 3 is a good fit for the structure observed in the seed dataset.\n",
    "\n",
    "Even so, other values of k will be tested to ensure robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Evaluation of K-Means with Different Values of k__\n",
    "\n",
    "In this section, the performance of the K-Means algorithm is evaluated using different values of k (number of clusters). This helps identify the optimal number of clusters that best represents the structure of the data.\n",
    "\n",
    "Plots are generated for k = 2 and k = 4, comparing the distribution of data in the PCA space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Results for K = 2\n",
    "y_predict_2 = KMeans(n_clusters=2, n_init=25, random_state=seed).fit_predict(X=X_final_pca)\n",
    "ax[0].scatter(\n",
    "    x = X_final_pca[:, 0],\n",
    "    y = X_final_pca[:, 1],\n",
    "    c = y_predict_2,\n",
    "    marker = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "ax[0].set_title('KMeans with K=2')\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Results for K = 4\n",
    "y_predict_4 = KMeans(n_clusters=4, n_init=25, random_state=seed).fit_predict(X=X_final_pca)\n",
    "ax[1].scatter(\n",
    "    x = X_final_pca[:, 0],\n",
    "    y = X_final_pca[:, 1],\n",
    "    c = y_predict_4,\n",
    "    marker = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "ax[1].set_title('KMeans with K=4')\n",
    "ax[1].set_xlabel('PC1')\n",
    "ax[1].set_ylabel('PC2')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- With **k = 2**, the data is divided into two large groups, losing important internal structures.\n",
    "- With **k = 4**, the clustering tends to oversegment the data, creating small clusters that do not correspond to natural groupings.\n",
    "- With **k = 3**, the data is divided into three compact and well-separated groups, without significant overlap or oversegmentation.\n",
    "\n",
    "The visual comparison confirms that **k = 3** is the most appropriate number of clusters for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
