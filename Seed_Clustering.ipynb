{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carlos Bravo GarrÃ¡n - 100474964"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Seed Clustering__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __1. Load the dataset__\n",
    "\n",
    "Load the seed dataset from a CSV file. \n",
    "\n",
    "The features are stored in `X`, and the target class labels are stored in `y`.\n",
    "\n",
    "Add seed variable for random states (`100474964`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/semillas.csv')\n",
    "X = data.drop(columns=['clase'])\n",
    "y = data['clase']\n",
    "\n",
    "seed = 100474964\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __2. Comparison of Scalers__\n",
    "\n",
    "This section identifies the most appropriate scaler for the seed dataset before applying clustering algorithms. Scaling ensures that all features contribute equally to the distance calculations.\n",
    "\n",
    "Three scalers are compared:\n",
    "- MinMaxScaler\n",
    "- RobustScaler\n",
    "- StandardScaler\n",
    "\n",
    "The scaled data is projected into 2D using PCA for visual evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler(),\n",
    "    'StandardScaler': StandardScaler()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "for i, (name, scaler) in enumerate(scalers.items(), 1):\n",
    "    pipeline = make_pipeline(scaler, PCA(n_components=2, random_state=seed))\n",
    "    \n",
    "    X_pca = pipeline.fit_transform(X)\n",
    "    \n",
    "    plt.subplot(1, 3, i)\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7, s=50)\n",
    "    plt.title(f'{name} + PCA')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.suptitle('Comparison of Scalers with PCA (By Seed Class)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Best Scaler Selection\n",
    "\n",
    "After observing the PCA plots:\n",
    "\n",
    "- **MinMaxScaler**: The data points are well distributed with moderate separation between different seed classes. Although some overlap exists, the distribution appears balanced and suitable for clustering.\n",
    "- **RobustScaler**: There is a good separation of classes, but the spread of the data is larger, which may not be ideal for density-based methods.\n",
    "- **StandardScaler**: While resistant to outliers, the classes are not well separated, making clustering more difficult.\n",
    "\n",
    "The scaler selected is **MinMaxScaler** because it provides a balanced and homogeneous distribution of the data, facilitating the identification of clusters without introducing large variations in scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Variance Explained by PCA\n",
    "\n",
    "To ensure that the 2D projection using PCA retains enough information from the original dataset, calculate, after applying each scaler, the variance explained by the two principal components and the total variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_ratios = {}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=2, random_state=seed)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    variance_explained = np.sum(pca.explained_variance_ratio_)\n",
    "    variance_ratios[name] = {\n",
    "        'PC1': pca.explained_variance_ratio_[0],\n",
    "        'PC2': pca.explained_variance_ratio_[1],\n",
    "        'Total': variance_explained\n",
    "    }\n",
    "\n",
    "variance_table = pd.DataFrame.from_dict(variance_ratios, orient='index')\n",
    "variance_table.index.name = 'Scaler'\n",
    "variance_table.reset_index(inplace=True)\n",
    "\n",
    "display(variance_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that all three scalers achieve a high variance explanation (>85%), indicating that the 2D PCA projection is representative of the original dataset in every case.\n",
    "\n",
    "Among them, **MinMaxScaler** achieves the highest variance explained, with 91.81%.\n",
    "\n",
    "It's confirmed that using **MinMaxScaler** is appropriate, as it retains the largest proportion of the original data variance after dimensionality reduction. Therefore, MinMaxScaler is selected for the clustering tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Scaling and PCA\n",
    "The dataset was scaled using the selecter scaler (**MinMaxScaler**) and reduced to two dimensions using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    PCA(n_components=2, random_state=seed)\n",
    ")\n",
    "\n",
    "X_final_pca = pipeline.fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(X_final_pca[:, 0], X_final_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "plt.title('MinMaxScaler + PCA (Final Preprocessing)')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('MinMaxScaler + PCA', fontsize=10)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __3. Clustering__\n",
    "\n",
    "Apply unsupervised clustering techniques on the PCA-transformed seed dataset to identify natural groupings:\n",
    "\n",
    "- K-Means\n",
    "- Hierarchical Clustering\n",
    "- DBSCAN\n",
    "\n",
    "For each method,the key hyperparameters are tuned, the results visualized and their performance compared to determine which best captures the structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3.1 K-means clustering__\n",
    "\n",
    "Based on the visual inspection of the PCA plot, k = 3 is initially selected as a reasonable number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Model creation and training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_kmeans = KMeans(\n",
    "    n_clusters=3,\n",
    "    n_init=25,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "modelo_kmeans.fit(X_final_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Cluster prediction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelo_kmeans.predict(X_final_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Cluster Visualization and Evaluation__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['yellow', 'purple', 'green', 'orange', 'purple']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "\n",
    "for cluster in np.unique(y_pred):\n",
    "    ax.scatter(\n",
    "        x = X_final_pca[y_pred == cluster, 0],\n",
    "        y = X_final_pca[y_pred == cluster, 1],\n",
    "        color = colors[cluster],\n",
    "        label = f\"Cluster {cluster}\",\n",
    "        edgecolor = 'black',\n",
    "        marker = 'o'\n",
    "    )\n",
    "\n",
    "ax.scatter(\n",
    "    x = modelo_kmeans.cluster_centers_[:, 0],\n",
    "    y = modelo_kmeans.cluster_centers_[:, 1],\n",
    "    c = 'red',\n",
    "    s = 200,\n",
    "    marker = '*',\n",
    "    label = 'Centroids'\n",
    ")\n",
    "\n",
    "ax.set_title('Clusters generated by KMeans (K=3)')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Means clustering with k = 3 produced three clearly defined groups in the PCA-transformed space.\n",
    "\n",
    "- Each cluster is compact and well-separated from the others, with minimal overlap between groups.\n",
    "- The centroids are located close to the center of each cluster, indicating that the algorithm has correctly captured the dense regions of the data.\n",
    "- The overall structure suggests that the data naturally forms three main groups, supporting the choice of k = 3.\n",
    "- Although the clusters are not perfectly spherical, the distribution around each centroid is balanced and coherent.\n",
    "\n",
    "In conclusion, the visual inspection confirms that K-Means with k = 3 is a good fit for the structure observed in the seed dataset.\n",
    "\n",
    "Even so, other values of k will be tested to ensure robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Evaluation of K-Means with Different Values of k__\n",
    "\n",
    "In this section, the performance of the K-Means algorithm is evaluated using different values of k (number of clusters). This helps identify the optimal number of clusters that best represents the structure of the data.\n",
    "\n",
    "Plots are generated for k = 2 and k = 4, comparing the distribution of data in the PCA space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Results for K = 2\n",
    "y_predict_2 = KMeans(n_clusters=2, n_init=25, random_state=seed).fit_predict(X=X_final_pca)\n",
    "ax[0].scatter(\n",
    "    x = X_final_pca[:, 0],\n",
    "    y = X_final_pca[:, 1],\n",
    "    c = y_predict_2,\n",
    "    marker = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "ax[0].set_title('KMeans with K=2')\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Results for K = 4\n",
    "y_predict_4 = KMeans(n_clusters=4, n_init=25, random_state=seed).fit_predict(X=X_final_pca)\n",
    "ax[1].scatter(\n",
    "    x = X_final_pca[:, 0],\n",
    "    y = X_final_pca[:, 1],\n",
    "    c = y_predict_4,\n",
    "    marker = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "ax[1].set_title('KMeans with K=4')\n",
    "ax[1].set_xlabel('PC1')\n",
    "ax[1].set_ylabel('PC2')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- With **k = 2**, the data is divided into two large groups, losing important internal structures.\n",
    "- With **k = 4**, the clustering tends to oversegment the data, creating small clusters that do not correspond to natural groupings.\n",
    "- With **k = 3**, the data is divided into three compact and well-separated groups, without significant overlap or oversegmentation.\n",
    "\n",
    "The visual comparison confirms that **k = 3** is the most appropriate number of clusters for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Elbow Method__\n",
    "\n",
    "To support the selection of the number of clusters, the elbow method is applied, which analyzes how the inertia decreases as the number of clusters increases.\n",
    "\n",
    "The optimal number of clusters is identified at the point where adding an additional cluster no longer significantly reduces the inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "K = range(1, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X_final_pca)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot of inertia facing the number of clusters shows a clear \"elbow\" at k = 3, reconfirming that k = 3 is the most appropriate number of clusters for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Silhouette Method__\n",
    "\n",
    "The silhouette coefficient measures how similar points within the same cluster are compared to those in other clusters. The closer to 1, the better defined the cluster.\n",
    "\n",
    "We will calculate and visualize the average silhouette coefficient for different values of k (from 2 to 10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "K = range(2, 11)\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=25, random_state=seed)\n",
    "    labels = kmeans.fit_predict(X_final_pca)\n",
    "    score = silhouette_score(X_final_pca, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(K, silhouette_scores, 'bo-')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.title('Silhouette Method for Optimal k')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that k = 2 has the highest silhouette coefficient (~0.56), suggesting good separation between two groups. However, this may be because the classes are divided into two large generic groups, ignoring more natural subdivisions.\n",
    "\n",
    "This differs from the choice we made earlier of k = 3, so to clarify, we will generate individual silhouette plots for k = 2 to k = 5 to see if the clusters are well-defined and how consistent the assignments are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [2, 3, 4, 5]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, k in enumerate(Ks):\n",
    "    ax = axes[idx]\n",
    "    kmeans = KMeans(n_clusters=k, n_init=25, random_state=seed)\n",
    "    cluster_labels = kmeans.fit_predict(X_final_pca)\n",
    "\n",
    "    silhouette_vals = silhouette_samples(X_final_pca, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(k):\n",
    "        ith_cluster_silhouette_vals = silhouette_vals[cluster_labels == i]\n",
    "        ith_cluster_silhouette_vals.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_vals.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / k)\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_vals,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7\n",
    "        )\n",
    "\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10 \n",
    "\n",
    "    avg_score = silhouette_score(X_final_pca, cluster_labels)\n",
    "    ax.text(0.7, 0.9, f'Avg Silhouette: {avg_score:.3f}', transform=ax.transAxes, color=\"red\", fontsize=10)\n",
    "\n",
    "\n",
    "    ax.axvline(x=avg_score, color=\"red\", linestyle=\"--\")\n",
    "    ax.set_title(f'Silhouette plot for K = {k}')\n",
    "    ax.set_xlabel('Silhouette coefficient values')\n",
    "    ax.set_ylabel('Cluster label')\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([-0.1, 1])\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Number of Clusters (k) | Average Silhouette | Analysis                                                                 |\n",
    "|------------------------|--------------------|-------------------------------------------------------------------------------|\n",
    "| 2                      | ~0.56           | Clear and compact structure, but may be an excessive simplification          |\n",
    "| 3                      | ~0.51           | Good separation, consistent with the actual number of classes (3)            |\n",
    "| 4                      | ~0.43           | Overclustering, appearance of small clusters and overlap between groups      |\n",
    "| 5                      | ~0.40           | Worse cohesion, some artificial clusters and poor point assignment           |\n",
    "\n",
    "\n",
    "The silhouette plots show that the values of n_clusters = 4 and 5 are a poor choice due to below-average silhouette scores and the appearance of clusters with very unequal sizes.\n",
    "\n",
    "On the other hand, the silhouette analysis is more ambiguous when comparing k = 2 and k = 3, as we have seen before. The value of k = 2 presents the highest average silhouette coefficient (~0.56), and its clusters are clearly separated, although the grouping may be too general, hiding more detailed structures. This can also be seen in the plot, where one of the clusters (cluster 0) is larger and encompasses multiple subsets.\n",
    "\n",
    "With k = 3, the average silhouette coefficient is slightly lower (~0.51), but the three clusters exhibit good internal cohesion and relative separation, with more balanced sizes and consistent bars. Additionally, k = 3 is consistent with the results obtained previously.\n",
    "\n",
    "In summary, although k = 2 achieves the highest average score, the visual and conceptual analysis suggests that k = 3 offers a richer, more interpretable segmentation aligned with the actual structure of the dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
